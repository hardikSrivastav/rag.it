
# PRD: Internal RAG System / Personal Knowledge Lake

## 1. Introduction & Vision

The Internal RAG (Retrieval-Augmented Generation) System, or "Personal Knowledge Lake," is the intelligent core of the entire internal tool suite. The vision is to create a single, searchable repository for the user's entire digital lifeâ€”emails, documents, notes, applications, and more. By leveraging a powerful RAG pipeline, this tool will allow the user to ask complex questions in natural language and receive synthesized, context-aware answers based on their own data, effectively creating a personalized, private search engine and memory assistant.

## 2. Goals

*   **Create a Unified Knowledge Base:** Ingest and index data from all other internal tools and local files into a single, queryable data lake.
*   **Enable Natural Language Querying:** Allow the user to ask questions and retrieve information as if talking to a personal assistant who has read everything.
*   **Provide Synthesized Answers:** Go beyond simple document retrieval by using an LLM to generate comprehensive answers based on multiple sources from the knowledge base.
*   **Serve as the "Memory" for All Tools:** Act as the central API for other tools to fetch deep, contextual information.

## 3. Core Features (MVP)

*   **File Upload & Ingestion:** A simple UI to upload local files (PDF, DOCX, TXT, MD).
*   **Core RAG Pipeline:**
    *   **Document Loading & Chunking:** Automatically load text from uploaded files and split them into smaller, semantically meaningful chunks.
    *   **Embedding & Indexing:** Convert each chunk into a vector embedding and store it in a Qdrant vector database.
    *   **Retrieval & Generation:** A chat interface where a user's query is used to retrieve the most relevant chunks from Qdrant, which are then passed to an LLM (along with the original query) to generate a final answer.
*   **API for Data Ingestion:** An API endpoint that other tools can call to push their data into the RAG system.
*   **Source Citation:** In the chat response, provide references to the original source documents that were used to generate the answer.

## 4. Extra/Advanced Features (Post-MVP)

*   **Automated Data Connectors:** Scheduled connectors that automatically pull data from various sources. This would turn the RAG system into a true personal knowledge lake. Potential connectors include:
    *   **Local File System:** A crawler to index local files and directories using a Merkle tree structure for efficient and robust change detection, similar to how version control systems like Git operate. This provides several benefits:
        *   **Efficient Change Detection:** Any change to a file or directory propagates up the tree, changing the root hash. Comparing the root hash is a quick way to see if anything has changed.
        *   **Content-Addressable Storage:** Files can be identified by the hash of their content, allowing for deduplication.
        *   **Integrity Verification:** The tree structure allows for verifying the integrity of the indexed files.
    *   **Implementation using Merkle Trees:**
        *   **Leaf Nodes:** Each file is a leaf node. Its hash is generated from its content and metadata (like size and modification time).
        *   **Internal Nodes:** Each directory is an internal node. Its hash is generated by combining the names and hashes of its children (files and subdirectories).
        *   **Root Hash:** The hash of the root directory uniquely represents the state of the entire indexed file system.
        *   **Indexing Process:** The system will recursively traverse the target directories, building the Merkle tree and storing the mapping of `path -> hash` in a database (like SQLite or a key-value store) for fast lookups. This approach is more robust than simple timestamp checking. For further reading on this concept, see the discussion on [maintaining Merkle Trees at the filesystem level](https://github.com/gnulug/merkle-tree-file-system).
    *   **Notion:** A connector to sync pages and databases from a Notion workspace. This would use the official Notion API.
    *   **GitHub:** A connector to clone and index code from specified repositories. This would allow asking questions about personal or work-related codebases.
    *   **Messaging Apps (Whatsapp, etc.):** A connector that can process exported chat histories. This would likely rely on a manual export from the user, but the system would automate the ingestion of the exported files.
    *   **Social Media (LinkedIn, X/Twitter):** Connectors to ingest personal activity, connections, messages, and posts. This would supplement the data from the "LinkedIn Post Engine."
    *   **Email:** A connector to sync emails from an email account, making them searchable. This would integrate with or replace the functionality of saving threads from the separate "Email Client" tool.
    *   **Specialized Apps:**
        *   **College/University Data:** Connectors for specific university portals for project data, grades, etc.
        *   **Health & Fitness Apps:** A connector for calorie tracking apps or other health data sources, to analyze personal habits and trends.
*   **Advanced RAG Strategies:**
    *   **Re-ranking:** Use a more powerful model to re-rank the initial retrieved documents for better relevance before sending them to the LLM.
    *   **Query Transformation:** Use an LLM to refine the user's initial query, making it more effective for vector search.
*   **OCR for Scanned Documents:** Integrate an Optical Character Recognition (OCR) engine (like Tesseract) to extract text from image-based PDFs or scanned documents.
*   **Multi-modal Support:** Ability to ingest and search for images based on their content.
*   **Proactive Insights:** A background agent that periodically analyzes the knowledge base to find interesting connections or surface timely reminders (e.g., "You have two documents about 'Project Alpha'. You should consider linking them.").

## 5. Technical Implementation Details

### Frontend (TypeScript / React / Shadcn)

*   **Architecture:** SPA using Vite + TypeScript.
*   **Key Components:**
    *   `ChatInterface.tsx`: A classic chat UI for asking questions and seeing responses. Will support streaming responses and rendering markdown.
    *   `FileUpload.tsx`: A drag-and-drop area for uploading documents.
    *   `DocumentManager.tsx`: A view to see all ingested documents, their status, and an option to re-index or delete them.
*   **State Management:** React Query for managing chat history and document lists. Zustand for UI state.

### Backend (FastAPI / Python)

This is the most complex backend of the suite.

*   **RAG Pipeline Implementation:**
    1.  **Loader (`/loaders`):** A module with different data loaders (e.g., `PyPDFLoader`, `UnstructuredFileLoader` from LangChain or LlamaIndex).
    2.  **Chunker (`/chunkers`):** Logic for text splitting. `RecursiveCharacterTextSplitter` is a good starting point.
    3.  **Embedder (`/embedders`):** A module to handle vector embedding generation. Use a high-quality open-source model like from `sentence-transformers` or a commercial API like OpenAI's.
    4.  **Vector Store (`/vector_stores`):** A dedicated module that acts as a wrapper around the `qdrant-client`. It will handle the logic for adding, updating, and querying vectors in Qdrant.
*   **Core Logic:**
    *   When a file is uploaded, it's passed through the loader, chunker, and embedder, and the final vectors (along with their text content and metadata) are stored in Qdrant and a corresponding metadata entry is made in SQLite.
    *   When a query comes in, the backend embeds the query, searches Qdrant for the top-k most similar chunks, constructs a detailed prompt containing these chunks and the user's question, and sends it to a powerful LLM (GPT-4, Gemini Pro) for generation.
*   **Frameworks:** Using a framework like **LangChain** or **LlamaIndex** can significantly accelerate development by providing pre-built components for many of these steps.
*   **API Endpoints:**
    *   `POST /api/ingest/upload`: The endpoint for file uploads.
    *   `POST /api/ingest/push`: The generic endpoint for other tools to push data (e.g., `{"source": "email_client", "content": "...", "metadata": {...}}`).
    *   `POST /api/chat`: The main endpoint for asking questions. This should support streaming responses back to the client.
    *   `GET /api/documents`: List all indexed documents.

### Data Storage (Qdrant + SQLite)

*   **Qdrant:**
    *   **Justification:** This is the heart of the RAG system. Qdrant is a production-ready, high-performance vector database perfect for this use case.
    *   **Usage:**
        *   A primary collection (e.g., `knowledge_lake`) will store all the vector embeddings.
        *   The payload of each vector will contain the raw text chunk and metadata like the `source_document_id`.
*   **SQLite (`db.sqlite3`):**
    *   **Justification:** To store metadata about the ingested source documents, keeping the vector database clean and focused on search.
    *   **Schema:**
        *   `documents`: (`id`, `filename`, `source_tool`, `status` (pending/indexed/failed), `created_at`, `metadata_json`).

## 6. Inter-tool Integrations

This tool primarily *receives* data from others, acting as the central sink.

*   **Email Client:** Important email threads can be "saved to knowledge lake," calling the `/api/ingest/push` endpoint.
*   **LinkedIn Post Engine:** All finalized posts and their research dumps can be automatically archived here. This allows querying past research, e.g., "What sources did I use for my post about AI in finance?".
*   **Internship Tracker:** All application data, including company names, roles, and job descriptions, will be ingested. This enables powerful queries like, "Give me a summary of all the 'Data Analyst' roles I applied to."
*   **Resume Updater:** Every version of the resume and all its master components will be stored here. This allows for historical analysis, e.g., "How did I describe my Ceneca.ai experience a year ago?". 